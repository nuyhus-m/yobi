# batch/batch_generator.py
"""
ë°°ì¹˜ ë¦¬í¬íŠ¸ ìƒì„± ëª¨ë“ˆ
"""

import asyncio
import aiohttp
import json
import time
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from contextlib import contextmanager
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker, Session
from services.health_data_service import HealthDataProcessor
from models import BatchLog

logger = logging.getLogger(__name__)

class BatchReportGenerator:
    """ë°°ì¹˜ ë¦¬í¬íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self, database_url: str, max_workers: int = 20, max_concurrent: int = 10):
        self.database_url = database_url
        self.max_workers = max_workers
        self.max_concurrent = max_concurrent
        self.engine = create_engine(database_url)
        self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
        
        # ì„±ëŠ¥ ì§€í‘œ
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_clients": 0,
            "success_count": 0,
            "error_count": 0,
            "errors": []
        }

# [ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ê´€ë¦¬ ë¸”ë¡] ì•ˆì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„± ë° ì²˜ë¦¬
# -------------------------------------------------------------------------
# ëª©ì :
# 1. ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ì˜ ì•ˆì „í•œ ìƒì„±
# 2. ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡¤ë°± ì²˜ë¦¬
# 3. ì„¸ì…˜ ìì›ì˜ ì•ˆì „í•œ í•´ì œ
    
    @contextmanager
    def get_db_session(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        db = self.SessionLocal()
        try:
            yield db
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()
# -------------------------------------------------------------------------           




# [ë°°ì¹˜ ë¦¬í¬íŠ¸ ìƒì„± ì›Œí¬í”Œë¡œìš° ë¸”ë¡] í´ë¼ì´ì–¸íŠ¸ ë¦¬í¬íŠ¸ ëŒ€ëŸ‰ ìƒì„± ë° ê´€ë¦¬
# -------------------------------------------------------------------------
# ëª©ì :
# 1. í™œì„± í´ë¼ì´ì–¸íŠ¸ ì‹ë³„ ë° í•„í„°ë§
# 2. ë³‘ë ¬ ë¦¬í¬íŠ¸ ìƒì„± í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬
# 3. ë°°ì¹˜ ì‘ì—… ì„±ê³¼ ì¶”ì  ë° í†µê³„ ìƒì„±
# 4. ë°°ì¹˜ ì‹¤í–‰ ê²°ê³¼ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
#   ì£¼ìš” ì›Œí¬í”Œë¡œìš°:
#       - ë³‘ë ¬ ë¦¬í¬íŠ¸ ìƒì„±
#       - í™œì„± í´ë¼ì´ì–¸íŠ¸ ì¡°íšŒ
#       - ê²°ê³¼ ì¢…í•© ë° í†µê³„ ê³„ì‚°
#       - ë°°ì¹˜ ë¡œê·¸ ì €ì¥
    

    async def generate_all_reports(self) -> Dict:
        """ëª¨ë“  í™œì„± í´ë¼ì´ì–¸íŠ¸ì˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.stats["start_time"] = time.time()
        logger.info("=== ë°°ì¹˜ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ ===")
        
        # í™œì„± í´ë¼ì´ì–¸íŠ¸ ì¡°íšŒ
        active_clients = await self.get_active_clients()
        self.stats["total_clients"] = len(active_clients)
        
        if not active_clients:
            logger.warning("í™œì„± í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return self._create_result_summary()
        
        # ë¹„ë™ê¸° HTTP ì„¸ì…˜ ìƒì„±
        connector = aiohttp.TCPConnector(limit=self.max_concurrent)
        timeout = aiohttp.ClientTimeout(total=60)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # ì„¸ë§ˆí¬ì–´: ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
            semaphore = asyncio.Semaphore(self.max_concurrent)
            
            async def process_with_semaphore(client_data):
                # ì„¸ë§ˆí¬ì–´ íšë“ â†’ ì‘ì—… ìˆ˜í–‰ â†’ ì„¸ë§ˆí¬ì–´ í•´ì œ
                async with semaphore:
                    return await self.generate_single_report(client_data, session) # âœ…ğŸƒë³¸ê²©ì ìœ¼ë¡œ ë¦¬í¬íŠ¸ ë§Œë“¤ê¸°!!
            
            # ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ì— ëŒ€í•´ ë™ì‹œ ì‹¤í–‰
            tasks = [process_with_semaphore(client) for client in active_clients]
            
            # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì½œë°±
            completed_tasks = 0
            async def progress_callback(task):
                nonlocal completed_tasks
                completed_tasks += 1
                progress = (completed_tasks / len(tasks)) * 100
                logger.info(f"ì§„í–‰ ìƒí™©: {completed_tasks}/{len(tasks)} ({progress:.1f}%)")
            
            # ì‘ì—… ì‹¤í–‰ ë° ì§„í–‰ ìƒí™© ì¶”ì 
            results = []
            for task in asyncio.as_completed(tasks):
                result = await task
                await progress_callback(task)
                results.append(result)
                
                # ê²°ê³¼ ë¶„ë¥˜
                if result.get("status") == "success":
                    self.stats["success_count"] += 1
                else:
                    self.stats["error_count"] += 1
        
        self.stats["end_time"] = time.time()
        
        # ê²°ê³¼ ìš”ì•½
        result_summary = self._create_result_summary()
        result_summary["details"] = results
        
        logger.info("=== ë°°ì¹˜ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ ===")
        logger.info(f"ì´ ì†Œìš”ì‹œê°„: {result_summary['total_duration']:.2f}ì´ˆ")
        logger.info(f"ì„±ê³µ: {self.stats['success_count']}, ì‹¤íŒ¨: {self.stats['error_count']}")
        
        # ë°°ì¹˜ ë¡œê·¸ ì €ì¥
        await self._save_batch_log(result_summary)
        
        return result_summary
    

    async def get_active_clients(self) -> List[Dict]:
        """í™œì„± í´ë¼ì´ì–¸íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        with self.get_db_session() as db:
            # ìµœê·¼ 7ì¼ ë‚´ì— ì¸¡ì • ë°ì´í„°ê°€ ìˆëŠ” í´ë¼ì´ì–¸íŠ¸ë§Œ ì¡°íšŒ
            seven_days_ago = int((time.time() - 7 * 24 * 3600) * 1000)
            
            query = text("""
                SELECT DISTINCT c.client_id, c.user_id, c.name
                FROM clients c 
                JOIN measure m ON c.client_id = m.client_id 
                WHERE m.date >= :seven_days_ago
                ORDER BY c.client_id
            """)
            
            result = db.execute(query, {"seven_days_ago": seven_days_ago})
            clients = [
                {
                    "client_id": row.client_id, 
                    "user_id": row.user_id, 
                    "name": row.name
                } 
                for row in result
            ]

            
            
            logger.info(f"í™œì„± í´ë¼ì´ì–¸íŠ¸ {len(clients)}ëª… ì¡°íšŒ ì™„ë£Œ")

            # return clients

            # client_idê°€ 1ì¸ í´ë¼ì´ì–¸íŠ¸ë§Œ ì°¾ì•„ì„œ ë°˜í™˜
            for client in clients:
                if client["client_id"] == 1:
                    logger.info(f"client_idê°€ 1ì¸ í´ë¼ì´ì–¸íŠ¸ ì¡°íšŒ ì™„ë£Œ")
                    return [client]
            
            # client_idê°€ 1ì¸ í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
            logger.info("client_idê°€ 1ì¸ í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None


    def _create_result_summary(self) -> Dict:
        """ë°°ì¹˜ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        total_duration = (self.stats["end_time"] - self.stats["start_time"]) if self.stats["end_time"] else 0
        
        return {
            "batch_start": self.stats["start_time"],
            "batch_end": self.stats["end_time"],
            "total_duration": total_duration,
            "total_clients": self.stats["total_clients"],
            "success_count": self.stats["success_count"],
            "error_count": self.stats["error_count"],
            "success_rate": (self.stats["success_count"] / self.stats["total_clients"] * 100) if self.stats["total_clients"] > 0 else 0,
            "average_time_per_client": total_duration / self.stats["total_clients"] if self.stats["total_clients"] > 0 else 0,
            "errors": self.stats["errors"]
        }
    
    async def _save_batch_log(self, result_summary: Dict):
        """ë°°ì¹˜ ì‹¤í–‰ ê²°ê³¼ ë¡œê·¸ ì €ì¥ (ì‹¤íŒ¨í•œ ê±´ ì €ì¥ì¥)"""
        try:
            
            with self.get_db_session() as db:
                # ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ ID ì¶”ì¶œ
                failed_client_ids = [
                    error['client_id'] 
                    for error in result_summary.get('errors', [])
                ]

                batch_log_data = {
                    "executed_at": int(result_summary["batch_start"] * 1000),
                    "total_clients": result_summary["total_clients"],
                    "success_count": result_summary["success_count"],
                    "error_count": result_summary["error_count"],
                    "duration": result_summary["total_duration"],
                    "success_rate": result_summary["success_rate"],
                    "failed_client_ids": failed_client_ids,  # ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ ID ì €ì¥
                    "details": json.dumps(result_summary, ensure_ascii=False)
                }
                
                # ì‹¤ì œ ë¡œê·¸ ì €ì¥ ë¡œì§
                query = text("""
                    INSERT INTO batch_logs (
                        executed_at, total_clients, success_count, 
                        error_count, duration, success_rate, 
                        failed_client_ids, details
                    ) VALUES (
                        :executed_at, :total_clients, :success_count, 
                        :error_count, :duration, :success_rate, 
                        :failed_client_ids, :details
                    )
                """)
                db.execute(query, batch_log_data)
                db.commit()
                
                logger.info("ë°°ì¹˜ ì‹¤í–‰ ë¡œê·¸ ì €ì¥ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
# -------------------------------------------------------------------------






# [ë‹¨ì¼ í´ë¼ì´ì–¸íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ë¸”ë¡] ê°œë³„ í´ë¼ì´ì–¸íŠ¸ ê±´ê°• ë¦¬í¬íŠ¸ ìƒì„± í”„ë¡œì„¸ìŠ¤
# -------------------------------------------------------------------------
# ëª©ì :
# 1. ê°œë³„ í´ë¼ì´ì–¸íŠ¸ì˜ ê±´ê°• ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
# 2. AI ê¸°ë°˜ ê±´ê°• ìƒíƒœ ì¢…í•© í‰ê°€
# 3. ì¼ì§€ ë°ì´í„° í†µí•©
# 4. OpenAIë¥¼ í™œìš©í•œ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
# 5. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
#
# ì£¼ìš” ì›Œí¬í”Œë¡œìš°:
#   - ê±´ê°• ë°ì´í„° ìˆ˜ì§‘
#   - ë‚´ë¶€ AI ë¶„ì„
#   - ì¼ì§€ ë°ì´í„° í†µí•©
#   - ìµœì¢… AI ë¶„ì„
#   - ë¦¬í¬íŠ¸ ì €ì¥
#
# ì˜¤ë¥˜ ì²˜ë¦¬:
#   - ê° ë‹¨ê³„ë³„ ì˜ˆì™¸ ìº¡ì²˜
#   - ì˜¤ë¥˜ ì •ë³´ ê¸°ë¡
#   - ë°°ì¹˜ í†µê³„ì— ì˜¤ë¥˜ ë°˜ì˜


    
    async def generate_single_report(self, client_data: Dict, session: aiohttp.ClientSession) -> Dict:

        """ë‹¨ì¼ í´ë¼ì´ì–¸íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        client_id = client_data['client_id']
        user_id = client_data['user_id']
        client_name = client_data.get('name', f'Client-{client_id}')
        
        start_time = time.time()
        
        try:
            with self.get_db_session() as db:
                # HealthDataProcessorë¥¼ ì§ì ‘ import (ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ)
                from services.health_data_service import HealthDataProcessor
                processor = HealthDataProcessor(db)
                
                # ë°°ì¹˜ ëª¨ë“œ ì„¤ì •
                processor.set_batch_mode(True, session) # aiohttp ì„¸ì…˜ ì „ë‹¬
                
                # 1. ê±´ê°• ë°ì´í„° ìˆ˜ì§‘
                health_data = await processor.collect_health_data(client_id, user_id)
                
                # 2. ë‚´ë¶€ AI í˜¸ì¶œ
                internal_ai_response = await processor.call_internal_ai(health_data)
                
                # 3. AI output ì •ì œ
                ai_summary = internal_ai_response
                
                # 4. ì¼ì§€ ë°ì´í„° ì¡°íšŒ
                journal_data = await processor.get_journal_data(client_id, user_id)
                
                # 5. OpenAI í˜¸ì¶œ
                openai_result = await processor.call_openai(ai_summary, journal_data)
                
                # 6. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                report_id = processor.save_to_database(client_id, user_id, ai_summary, openai_result)
                
                end_time = time.time()
                duration = end_time - start_time
                
                logger.info(f"ë¦¬í¬íŠ¸ ìƒì„± ì„±ê³µ - {client_name} (Client ID: {client_id}), ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ")
                
                return {
                    "client_id": client_id,
                    "client_name": client_name,
                    "report_id": report_id,
                    "status": "success",
                    "duration": duration,
                    "health_records": len(health_data.get("dm", [])),
                    "journal_entries": len(journal_data),
                    "anomalies": len(ai_summary.get("anom", []))
                }
                
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            error_info = {
                "client_id": client_id,
                "client_name": client_name,
                "error": str(e),
                "error_type": type(e).__name__,
                "duration": duration
            }
            
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ - {client_name} (Client ID: {client_id}): {str(e)}")
            self.stats["errors"].append(error_info)
            
            return {
                "client_id": client_id,
                "client_name": client_name,
                "status": "error",
                "error": str(e),
                "duration": duration
            }
    
    
# -------------------------------------------------------------------------



# [ë°°ì¹˜ ì‹¤íŒ¨ í´ë¼ì´ì–¸íŠ¸ ì¬ì²˜ë¦¬ ë¸”ë¡] ê³¼ê±° ë°°ì¹˜ ì‘ì—…ì˜ ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ ì¬ì‹œë„
# -------------------------------------------------------------------------
# ëª©ì :
# 1. ì´ì „ ë°°ì¹˜ ì‘ì—…ì—ì„œ ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ ì‹ë³„
# 2. ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ ê°œë³„ ë¦¬í¬íŠ¸ ì¬ìƒì„±
# 3. ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ ë° ë³µêµ¬
#
# ì£¼ìš” ì›Œí¬í”Œë¡œìš°:
#   - ë°°ì¹˜ ë¡œê·¸ì—ì„œ ì‹¤íŒ¨ í´ë¼ì´ì–¸íŠ¸ ID ì¶”ì¶œ
#   - í´ë¼ì´ì–¸íŠ¸ ì •ë³´ ì¡°íšŒ
#   - ê°œë³„ ë¦¬í¬íŠ¸ ì¬ìƒì„±
#   - ì¬ì²˜ë¦¬ ê²°ê³¼ ìˆ˜ì§‘
#
# ì˜¤ë¥˜ ì²˜ë¦¬:
#   - ê°œë³„ í´ë¼ì´ì–¸íŠ¸ ì‹¤íŒ¨ ì‹œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨ ë°©ì§€
#   - ì‹¤íŒ¨ ë¡œê·¸ ê¸°ë¡

    async def retry_failed_clients(self, batch_log_id: int):
        """íŠ¹ì • ë°°ì¹˜ì˜ ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ ì¬ì²˜ë¦¬"""
        with self.get_db_session() as db:
            # ë°°ì¹˜ ë¡œê·¸ ì¡°íšŒ
            batch_log = db.query(BatchLog).filter(BatchLog.id == batch_log_id).first()
            
            if not batch_log:
                raise ValueError("í•´ë‹¹ ë°°ì¹˜ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ ID ì¶”ì¶œ
            failed_client_ids = json.loads(batch_log.failed_client_ids)

            # ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ë“¤ ì¬ì²˜ë¦¬
            retry_results = []
            for client_id in failed_client_ids:
                try:
                    # í´ë¼ì´ì–¸íŠ¸ ì •ë³´ ì¡°íšŒ
                    client_data = self._get_client_data(client_id)
                    
                    # ë‹¨ì¼ í´ë¼ì´ì–¸íŠ¸ ë¦¬í¬íŠ¸ ì¬ìƒì„±
                    async with aiohttp.ClientSession() as session:
                        result = await self.generate_single_report(client_data, session)
                    retry_results.append(result)
                
                except Exception as e:
                    logger.error(f"í´ë¼ì´ì–¸íŠ¸ {client_id} ì¬ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            
            return retry_results

    def _get_client_data(self, client_id: int) -> Dict:
        """í´ë¼ì´ì–¸íŠ¸ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ"""
        with self.get_db_session() as db:
            query = text("""
                SELECT client_id, user_id, name 
                FROM clients 
                WHERE client_id = :client_id
            """)
            result = db.execute(query, {"client_id": client_id}).first()
            
            if not result:
                raise ValueError(f"í´ë¼ì´ì–¸íŠ¸ ID {client_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            return {
                "client_id": result.client_id,
                "user_id": result.user_id,
                "name": result.name
            }
# -------------------------------------------------------------------------





    async def generate_single_client_report(self, client_id: int) -> Dict:
        """ë‹¨ì¼ í´ë¼ì´ì–¸íŠ¸ì˜ ë¦¬í¬íŠ¸ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
        with self.get_db_session() as db:
            client_query = text("SELECT client_id, user_id, name FROM clients WHERE client_id = :client_id")
            result = db.execute(client_query, {"client_id": client_id}).first()
            
            if not result:
                raise ValueError(f"í´ë¼ì´ì–¸íŠ¸ ID {client_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            client_data = {
                "client_id": result.client_id,
                "user_id": result.user_id,
                "name": result.name
            }
            
            async with aiohttp.ClientSession() as session:
                return await self.generate_single_report(client_data, session)
    
    def get_current_stats(self) -> Dict:
        """í˜„ì¬ ì§„í–‰ ìƒí™© ì¡°íšŒ"""
        return {
            "is_running": self.stats["end_time"] is None and self.stats["start_time"] is not None,
            "elapsed_time": (time.time() - self.stats["start_time"]) if self.stats["start_time"] else 0,
            "total_clients": self.stats["total_clients"],
            "success_count": self.stats["success_count"],
            "error_count": self.stats["error_count"],
            "remaining": self.stats["total_clients"] - self.stats["success_count"] - self.stats["error_count"]
        }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    async def test_batch_generation():
        from core.config import settings
        
        generator = BatchReportGenerator(
            database_url=settings.DATABASE_URL,
            max_workers=10,
            max_concurrent=5
        )
        
        # ë‹¨ì¼ í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸
        # result = await generator.generate_single_client_report(1)
        # print(f"ë‹¨ì¼ í´ë¼ì´ì–¸íŠ¸ ê²°ê³¼: {result}")
        
        # ì „ì²´ ë°°ì¹˜ ì‹¤í–‰
        result = await generator.generate_all_reports()
        print(f"ë°°ì¹˜ ê²°ê³¼: {result}")
    
    asyncio.run(test_batch_generation())